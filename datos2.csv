programas
"import re
import nltk
import urllib
from nltk.corpus import stopwords
nltk.download('stopwords')
from bs4 import BeautifulSoup
from nltk.stem.porter import PorterStemmer
from urllib.request import urlopen
import pandas as pd

#defining header
header= {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) ' 
      'AppleWebKit/537.11 (KHTML, like Gecko) '
      'Chrome/23.0.1271.64 Safari/537.11',
      'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,/;q=0.8',
      'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',
      'Accept-Encoding': 'none',
      'Accept-Language': 'en-US,en;q=0.8',
      'Connection': 'keep-alive'}

#the URL where you are requesting at
req = urllib.request.Request(url=""https://towardsdatascience.com/automated-text-classification-using-machine-learning-3df4f4f9570b"", headers=header) 
file = urllib.request.urlopen(req)
html = file.read()
file.close()
soup = BeautifulSoup(html)
tit = []

for links in soup.find_all('p'):
    if links.string != None :
        tit.append(links.string)

#Numero de Titulos
n = 5
        
titulos = tit[2:n]
data = tit[2:4]
print('Prueba:',len(data))
df = pd.DataFrame(data, columns=['PARRAFOS'])
df.to_csv('Prueba1.csv', sep=',')

datos=pd.read_csv('Prueba1.csv', header=0)
titu=datos['PARRAFOS']

#Normalizacion
l = {}
for x in range(0,len(titu)):
    l[x] = titu[x]

d=list(l.values())
print(""------------------------------------------TITULOS-----------------------------------------"") 
print(d)  

def normalizar(n):
    n_d = re.sub('[^A-Za-z0-9]+', ' ', n)
    n2_d = n_d.lower()
    return n2_d



todas = []

def vocabulary(d):
    for item in d:
        if item not in todas:
            todas.append(item)
                


#Eliminacion de Stopwords
ne = stopwords.words('english')

nostop = []


def parapalabras(n):
    for word in n:
        if word not in ne:
            nostop.append(word)
            
            
p = {}
for x in range(0,len(titu)):
    p[x] = []

titulares=list(p.values())


def parapalabras2(n,num):
    for word in n:
        if word not in ne:
            titulares[num].append(word)

            
#Steamming            
vocabulario = []

steammer = PorterStemmer()
def stmm(n):
    for i in n:
        vocabulario.append(steammer.stem(i))

        
q = {}
for x in range(0,len(titu)):
    q[x] = []

titulos=list(q.values())

steammer = PorterStemmer()
def stmm2(n,num):
    for i in n:
        titulos[num].append(steammer.stem(i))
        

        
        
for x in range(0,len(titu)):   
    d[x] = normalizar(d[x])
    d[x] = d[x].split()   #Tokenizacion
    vocabulary(d[x])
    parapalabras2(d[x],x)
    stmm2(titulares[x],x)

parapalabras(todas)
stmm(nostop)





print(""-----------------------------------------VOCABULARIO-----------------------------------------"") 
print(vocabulario) 
print(""----------------------------------------------------------------------------------------------------"")

s = {}
for x in range(0,len(titu)):
    s[x] = []

ubicaciones=list(s.values())




for word in vocabulario:
    
    def ubicacion(posicion,n,ubicacion):
        for x in range(len(n)):
            if n[x] == word:
                ubicacion.append(x+1)
        print(""["",posicion,"","",n.count(word),"","",ubicacion,""]"","" | "",end = """")
        
    
    print(word,"" | "",end = """")
    i = 0
    
    for item in range(0,len(titu)):
    
        if word in titulos[item]:
            i += 1
            ubicacion(item+1,titulos[item],ubicaciones[item])
            ubicaciones[item] = []
    print("" | ni = "",i)
    print(""----------------------------------------------------------------------------------------------------"")
    
    
B = len(vocabulario)                                #Tamao Tabla HASH
lv = ['none'] * B                     #Tabla HASH con todos sus elementos 'none'
lvCod = ['none'] * B                  #Tabla HASH con todos sus elementos 'none' para vector de cdigos

v = vocabulario                       #Elementos a insertar

for i in v:                  
  ascii = 5*pow(len(i),2)+3*len(i)+1
  pos = ascii%B                  #MOD (Funcin HASH)
  while lv[pos] != 'none':            #Mientras la posicin no tenga un elemento 'none' realiza Redispersin
    k = (ascii%(B-1))+1
    pos = (pos+k)%B 
  else:                               #Al encontrar 'none' en una posicin, ubica el elemento
    lv[pos] = i
    lvCod[pos] = ascii

print(lv)                             #Imprime la tabla de palabras en forma de vector
print('------Vector de representaciones------')
print(lvCod)                          #Imprime la tabla de cdigos en forma de vector
ordenados = sorted(lvCod)
print(ordenados)
vector1 = []
for word in ordenados:
        if word not in vector1:
            vector1.append(word)
print(vector1)
vector2 = vector1[-6:]
print(vector2)

B = 10                                #Tamao Tabla HASH
lv = ['none'] * B                     #Tabla HASH con todos sus elementos 'none'
lvCod = ['none'] * B                  #Tabla HASH con todos sus elementos 'none' para vector de cdigos

v = vector2                       #Elementos a insertar

for i in v:                  
  ascii = 5*pow(i,2)+3*i+1
  pos = ascii%B                  #MOD (Funcin HASH)
  while lv[pos] != 'none':            #Mientras la posicin no tenga un elemento 'none' realiza Redispersin
    k = (ascii%(B-1))+1
    pos = (pos+k)%B 
  else:                               #Al encontrar 'none' en una posicin, ubica el elemento
    lv[pos] = i
    lvCod[pos] = ascii

print(lv)                             #Imprime la tabla de palabras en forma de vector
print('-----Palabras con representacin-----')
for word in range(0, B):
  if lv[word] != 'none':
    print('La palabra',lv[word],'tiene un valor de',lvCod[word])
print('-------------------------------------')
for x in range(0, B):     
    print(x,'-->',lv[x],'-->',lvCod[x])              #Imprime la tabla con posiciones"
"import tkinter as tk
import pandas as pd
import numpy as np
import math
import time
import csv
import io
import re
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
from bs4 import BeautifulSoup
from nltk.stem.porter import PorterStemmer
from tkinter import filedialog
from pathlib import Path
from numpy import dot
from numpy.linalg import norm
from scipy import spatial

start_time = time.time()
#1. Consumir en Python, directamente desde la URL, el siguiente dataset: https://archive.ics.uci.edu/ml/datasets/AAAI+2013+Accepted+Papers (Enlaces a un sitio externo.). 
#En el caso de no lograr hacer este paso ir al literal 2, caso contrario avanzar al literal 3 (1 Pto.)

url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00314/%5bUCI%5d%20AAAI-13%20Accepted%20Papers%20-%20Papers.csv'
df = pd.read_csv(url)
print(""----------------------Literal 1----------------------"")
print(df)
#######################################NORMALIZACION#######################################
def normalizacion(dato):
  titles=df[dato][0:6]

  titu=titles.values.tolist()

  #3. Realizar el proceso NLP de normalizacin de los abstracts, de los primeros seis documentos (0.5 Ptos.).
  #Normalizacion
  l = {}
  for x in range(0,len(titu)):
      l[x] = titu[x]

  d=list(l.values())

  def normalizar(n):
      n_d = re.sub('[^A-Za-z0-9]+', ' ', n)
      n2_d = n_d.lower()
      return n2_d


  todas = []

  def vocabulary(d):
      for item in d:
          if item not in todas:
              todas.append(item)
                  
  #4. Realizar eliminacin de stopwords, stemming (algoritmo de Porter) y tokenizacin de los abstracts, de los primeros seis documentos (0.5 Ptos.).
  #Eliminacion de Stopwords
  ne = stopwords.words('english')

  nostop = []


  def parapalabras(n):
      for word in n:
          if word not in ne:
              nostop.append(word)
              
              
  p = {}
  for x in range(0,len(titu)):
      p[x] = []

  titulares=list(p.values())


  def parapalabras2(n,num):
      for word in n:
          if word not in ne:
              titulares[num].append(word)

              
  #Steamming            
  subVocabulario = []

  steammer = PorterStemmer()
  def stmm(n):
      for i in n:
          subVocabulario.append(steammer.stem(i))

          
  q = {}
  for x in range(0,len(titu)):
      q[x] = []

  titulos=list(q.values())

  steammer = PorterStemmer()
  def stmm2(n,num):
      for i in n:
          titulos[num].append(steammer.stem(i))
  
  for x in range(0,len(titu)):
      d[x] = normalizar(d[x])
      d[x] = d[x].split()
      if dato==""Abstract"":
        vocabulary(d[x])
      parapalabras2(d[x],x)
      stmm2(titulares[x],x)


  parapalabras(todas)
  stmm(nostop)

  vocabulario = list(set(subVocabulario))
  print(""----------------------Literal 3----------------------"")
  print(d)
  print(""----------------------Literal 4----------------------"")
  print(vocabulario)
  #5. Obtener el full inverted index, del literal anterior (1 Pto.)
  print(""----------------------Literal 5----------------------"")
  s = {}
  for x in range(0,len(titu)):
      s[x] = []

  ubicaciones=list(s.values())

  for word in vocabulario:
    
    def ubicacion(posicion,n,ubicacion):
        for x in range(len(n)):
            if n[x] == word:
                ubicacion.append(x+1)
        print(""["",posicion,"","",n.count(word),"","",ubicacion,""]"","" | "",end = """")

    print(word,"" | "",end = """")
    i = 0
    
    for item in range(0,len(titu)):
    
        if word in titulos[item]:
            i += 1
            ubicacion(item+1,titulos[item],ubicaciones[item])
            ubicaciones[item] = []
    print("" | ni = "",i)
    print(""----------------------------------------------------------------------------------------------------"")
        
  return titulos,d,vocabulario,titu

def metodoTablaHash(vocabulario):
      #6. Colocar en un tabla hash de tamao 10, los primeros 5 tokens del literal anterior. Usar el sistema de codificacin ASCII, con base a=3 y funcin polinomial  (1 Pto.)
    literal5tokens = vocabulario[:5]

    a = 3                                 #Valor constante mayor a 1
    B = 10                                #Tamao Tabla HASH
    lv = ['none'] * B                     #Tabla HASH con todos sus elementos 'none'
    lvCod = ['none'] * B                  #Tabla HASH con todos sus elementos 'none' para vector de cdigos

    v = literal5tokens           #Elementos a insertar

    for i in v:                  
      acumulador = 0                      #Inicializar acumulador de la funcin
      j = len(i)-1                        #Menorar un valor a la longitud de la palabra
      for caracter in i:
        ascii = ord(caracter)             #Convertir a valor ASCII cada caracter de la palabra
        fc = ascii*pow(a,j)               #Aplicar funcin polinomial
        acumulador = acumulador + fc      #Sumar el valor de cada caracter hallado por la funcin polinomial
        j =  j-1                          #Restar cada iteracin a la longitud de la palabra
        
      pos = acumulador%B                  #MOD (Funcin HASH)
      while lv[pos] != 'none':            #Mientras la posicin no tenga un elemento 'none' realiza Redispersin
        k = (acumulador%(B-1))+1
        pos = (pos+k)%B 
      else:                               #Al encontrar 'none' en una posicin, ubica el elemento
        lv[pos] = i
        lvCod[pos] = acumulador

    print(""----------------------Literal 6----------------------"")
    print(lv)                             #Imprime la tabla de palabras en forma de vector
    print('------Vector de representaciones------')
    print(lvCod)                          #Imprime la tabla de cdigos en forma de vector
    print('-----Palabras con representacin-----')
    for word in range(0, B):
      if lv[word] != 'none':
        print('La palabra',lv[word],'tiene un valor de',lvCod[word])

#######################################MTODO JACCARD#######################################
def metodoJaccard(titulos,d):
  interseccion = []
  union = []
  coeficientes = []
  ######################################INTERSECCION###############################
  def matrizInterseccion(cadaTitulo):
    a = cadaTitulo
    b = cadaTitulo
    for i in a:
        for j in b:
            d = len(list(set(j) & set(i)))
            interseccion.append(d)
  ######################################SUMA DE ELEMENTOS######################################
  def matrizUnion(cadaTitulo):
    a = cadaTitulo
    b = cadaTitulo
    for i in a:
      for j in b:
        d = len(list(set(j).union(set(i))))
        union.append(d)
  
  matrizInterseccion(titulos)
  matrizInterseccionTitulos = np.reshape(interseccion,(len(d), len(d)))

  matrizUnion(titulos)
  matrizUnionTitulos = np.reshape(union,(len(d), len(d)))
  ######################################MATRIZ DE JACCARD###############################
  def jaccard(vectorUnion,vectorInterseccion):
    for i in range(len(vectorUnion)):
      coeficiente = vectorInterseccion[i]/vectorUnion[i]
      coeficientes.append(round(coeficiente,2))

  jaccard(union,interseccion)
  coeficienteJaccard = np.reshape(coeficientes,(len(d), len(d)))
  
  return coeficienteJaccard

#######################################MTODO DE COSENOS#######################################
def metodoCoseno(vocabulario,titu,titulos):
  TF_vector=[]
  WTF_vector=[]
  DF_vector=[]
  IDF_vector = []
  WTDxIDF_vector = []
  vectorSumaColumnas = []
  vectorNormalizado = []
  valores = []
  vectorIncidenciaBinaria = []

  ##########CALCULO DE TF##########
  for word in vocabulario:
    for n in titulos:
      TF_vector.append(n.count(word))

  ##########CALCULO DE WTF##########
  for item in TF_vector:
    if item>0:
      factor_WTF = 1+(math.log10(item))
      WTF_vector.append(factor_WTF)
    else:
      WTF_vector.append(0)

  WTF_matriz=np.reshape(WTF_vector,(len(vocabulario),len(titu)))

  ##########CALCULO DE DF##########
  for word in vocabulario:
    i=0
    for item in range(0,len(titu)):
      if word in titulos[item]:
        i += 1
        vectorIncidenciaBinaria.append(1)
      else: 
        vectorIncidenciaBinaria.append(0)
    DF_vector.append(i)

  #7. Obtener la matriz de incidencia binaria, del literal 4, e indicar las dimensiones de la matriz resultante (1 Pto.)
  matrizIncidenciaBinaria = np.reshape(vectorIncidenciaBinaria,(len(vocabulario),len(titu)))
  print(""----------------------Literal 7----------------------"")
  print(matrizIncidenciaBinaria)
  print(""Dimensiones de la Matriz de Incidencia Binaria:"",matrizIncidenciaBinaria.shape)
  ##########CALCULO DEL IDF##########
  for item in DF_vector:
    factor_ITF = math.log10(len(titu)/item)
    IDF_vector.append(factor_ITF)


  ##########CALCULO WTFxIDF##########
  for x in range(len(vocabulario)):
    for y in range(len(titu)):
      WTDxIDF_vector.append(IDF_vector[x]*WTF_matriz[x][y])

  WTDxIDF_matriz=np.reshape(WTDxIDF_vector,(len(vocabulario),len(titu)))
  #8. Obtener la matriz TF-IDF, del literal 4, e indicar las dimensiones de la matriz resultante (1 Pto.)
  print(""----------------------Literal 8----------------------"")
  print(WTDxIDF_matriz)
  print(""Dimensiones de la Matriz TF-IDF:"",WTDxIDF_matriz.shape)

  ##########NORMALIZACION DE VECTORES##########
  for i in range(len(titu)):
      acum  = 0
      for fila in WTDxIDF_matriz:
          acum = acum + pow(fila[i],2)
      vectorSumaColumnas.append(pow(acum,1/2))

  for x in range(len(vectorSumaColumnas)):
    for y in range(len(WTDxIDF_matriz)):
      vectorNormalizado.append(WTDxIDF_matriz[y][x]/vectorSumaColumnas[x])     

  matrizNormalizada = np.reshape(vectorNormalizado,(len(vectorSumaColumnas),len(WTDxIDF_matriz)))
  matrizTranspuesta = np.transpose(matrizNormalizada)

  TRANSPUESTA_dataframe=pd.DataFrame(matrizTranspuesta)

  ##########MTODO DEL SIMILITUD COSENO##########
  for i in range(len(titu)):
    for j in range(len(titu)):
      if j>=i:
        factor = round(TRANSPUESTA_dataframe[i].dot(TRANSPUESTA_dataframe[j]),3)
        valores.append(factor)
      else:
        valores.append(0)
      
  valores = np.reshape(valores,(len(titu),len(titu)))
  valores = valores + valores.T - np.diag(np.diag(valores))

  return valores
#######################################MATRIZ DE SIMILITUD CON DESIGNACIN DE PESOS#######################################
def similitud(matrizTitulos,matrizKeywords,matrizAbstract):
  #PESO PARA TTULOS 20% (0.2)
  titulosPeso = np.multiply(matrizTitulos,0.2)
  #PESO PARA KEYWORDS 30% (0.3)
  keywordsPeso = np.multiply(matrizKeywords,0.3)
  #PESO PARA ABSTRACT 50% (0.5)
  abstractPeso = np.multiply(matrizAbstract,0.5)
  #print(titulosPeso)
  #print(""---------------------------"")
  #print(keywordsPeso)
  #print(""---------------------------"")
  #print(abstractPeso)
  #print(""---------------------------"")
  
  matrizSimilitudPrevia = np.add(titulosPeso,keywordsPeso)
  matrizSimilitud = np.add(matrizSimilitudPrevia,abstractPeso)

  SIMILITUD_dataframe=pd.DataFrame(matrizSimilitud)

  return SIMILITUD_dataframe

print(""--------------------MATRIZ DE SIMILITUDES CON MTODO COSENO PARA ABSTRACT--------------------"")
[titulos,d,vocabulario,titu] = normalizacion(""Abstract"")
tablaHash = metodoTablaHash(vocabulario)
matrizAbstractJaccard = metodoJaccard(titulos,d)
print(""--------------------MATRIZ DE SIMILITUDES CON DESIGNACIN DE PESOS--------------------"")
matrizAbstractCoseno = metodoCoseno(vocabulario,titu,titulos)
#9. Mediante el coeficiente de Jaccard, en funcin de los abstracts que han pasado todo el proceso de NLP, indique que tan similares son los documentos D6 y D5 (1 Pto.)
print(""----------------------Literal 9----------------------"")
print(matrizAbstractJaccard[4][5])

print(""Tiempo total de ejecucin: %s segundos"" % (time.time() - start_time))"
"import tkinter as tk
import pandas as pd
import numpy as np
import math
import time
import csv
import io
import re
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
from bs4 import BeautifulSoup
from nltk.stem.porter import PorterStemmer
from tkinter import filedialog
from pathlib import Path
from numpy import dot
from numpy.linalg import norm
from scipy import spatial

start_time = time.time()

#Datos del ejercicio:
D1 = ""Fall Detection in Video Sequences Based on a Three-Stream Convolutional Neural Network.""
D2 = ""Moving Cast Shadow Detection in Video Based on New Chromatic Criteria and Statistical Modeling.""
D3 = ""Rare Geometries: Revealing Rare Categories via Dimension-driven Statistics.""
D4 = ""Pelee-Text: A Tiny Convolutional Neural Network for Multi-Oriented Scene Text Detection ""
D5 = ""Semi-Supervised Discriminative Transfer Learning in Cross-Language Text Classification.""
D6 = ""In cancer biology experiments, we use our algorithm to predict mutation status of important cancer genes from gene expression profiles using two distinct cancer populations, namely, patient-derived primary tumor data and in-vitro-derived cancer cell line data. We show that we can increase our generalization performance on primary tumors using cell lines as an auxiliary data source.""
D7 = ""Our two main contributions are: (i) two novel probabilistic models for binary and multiclass classification, and (ii) very efficient variational approximation procedures for these models. We illustrate the generalization performance of our algorithms on two different applications. In computer vision experiments, our method outperforms the state-of-the-art algorithms on nine out of 12 benchmark supervised domain adaptation experiments defined on two object recognition data sets.""
D8 = ""Transfer learning considers related but distinct tasks defined on heterogenous domains and tries to transfer knowledge between these tasks to improve generalization performance. It is particularly useful when we do not have sufficient amount of labeled training data in some tasks, which may be very costly, laborious, or even infeasible to obtain. Instead, learning the tasks jointly enables us to effectively increase the amount of labeled training data. In this paper, we formulate a kernelized Bayesian transfer learning framework that is a principled combination of kernel-based dimensionality reduction models with task-specific projection matrices to find a shared subspace and a coupled classification model for all of the tasks in this subspace.""

Q1 = ""Probabilisitic model in cancer experiments""
Q2 = ""Machine learning with smaller Datasets""
Q3 = ""Learning in tasks with small data and Classifications models""

datos = [D1,D2,D3,D4,D5,D6,D7,D8,Q1,Q2,Q3]
df = pd.DataFrame()
df['dato'] = datos

print(df)
#######################################NORMALIZACION#######################################
def normalizacion(dato):
  titles=df[dato]

  titu=titles.values.tolist()

  #1. Realizar un proceso de limpieza eliminacin de caracteres especiales y conversin a letras minsculas, tanto para consultas como para documentos. (0.5 Pto.)
  #Normalizacion
  l = {}
  for x in range(0,len(titu)):
      l[x] = titu[x]

  d=list(l.values())

  def normalizar(n):
      n_d = re.sub('[^A-Za-z0-9]+', ' ', n)
      n2_d = n_d.lower()
      return n2_d


  todas = []

  def vocabulary(d):
      for item in d:
          if item not in todas:
              todas.append(item)
                  
  #2. Eliminar las stopwords sobre los datos curados, realizar stemming (algoritmo de Porter) y tokenizar (tanto consultas como documentos). (0.5 Pto.).
  #Eliminacion de Stopwords
  ne = stopwords.words('english')

  nostop = []


  def parapalabras(n):
      for word in n:
          if word not in ne:
              nostop.append(word)
              
              
  p = {}
  for x in range(0,len(titu)):
      p[x] = []

  titulares=list(p.values())


  def parapalabras2(n,num):
      for word in n:
          if word not in ne:
              titulares[num].append(word)

              
  #Steamming            
  subVocabulario = []

  steammer = PorterStemmer()
  def stmm(n):
      for i in n:
          subVocabulario.append(steammer.stem(i))

          
  q = {}
  for x in range(0,len(titu)):
      q[x] = []

  titulos=list(q.values())

  steammer = PorterStemmer()
  def stmm2(n,num):
      for i in n:
          titulos[num].append(steammer.stem(i))
  
  for x in range(0,len(titu)):
      d[x] = normalizar(d[x])
      d[x] = d[x].split()
      if dato==""dato"":
        vocabulary(d[x])
      parapalabras2(d[x],x)
      stmm2(titulares[x],x)


  parapalabras(todas)
  stmm(nostop)

  vocabulario = list(set(subVocabulario))
  print(""----------------------Literal 1----------------------"")
  print(d)
  print(""----------------------Literal 2----------------------"")
  print(vocabulario)
        
  return titulos,d,vocabulario,titu

#######################################MTODO JACCARD#######################################
def metodoJaccard(titulos,d):
  interseccion = []
  union = []
  coeficientes = []
  ######################################INTERSECCION###############################
  def matrizInterseccion(cadaTitulo):
    a = cadaTitulo
    b = cadaTitulo
    for i in a:
        for j in b:
            d = len(list(set(j) & set(i)))
            interseccion.append(d)
  ######################################SUMA DE ELEMENTOS######################################
  def matrizUnion(cadaTitulo):
    a = cadaTitulo
    b = cadaTitulo
    for i in a:
      for j in b:
        d = len(list(set(j).union(set(i))))
        union.append(d)
  
  matrizInterseccion(titulos)
  matrizInterseccionTitulos = np.reshape(interseccion,(len(d), len(d)))

  matrizUnion(titulos)
  matrizUnionTitulos = np.reshape(union,(len(d), len(d)))
  ######################################MATRIZ DE JACCARD###############################
  def jaccard(vectorUnion,vectorInterseccion):
    for i in range(len(vectorUnion)):
      coeficiente = vectorInterseccion[i]/vectorUnion[i]
      coeficientes.append(round(coeficiente,2))

  jaccard(union,interseccion)
  coeficienteJaccard = np.reshape(coeficientes,(len(d), len(d)))
  
  return coeficienteJaccard

#######################################MTODO DE COSENOS#######################################
def metodoCoseno(vocabulario,titu,titulos):
  TF_vector=[]
  TF_matriz=[]
  WTF_vector=[]
  DF_vector=[]
  IDF_vector = []
  WTDxIDF_vector = []
  vectorSumaColumnas = []
  vectorNormalizado = []
  valores = []
  
  #3. Crear una sola bolsa de palabras unificada (tanto de consultas como documentos) e indicar las dimensiones de la matriz (1 Ptos.)
  ##########CALCULO DE TF##########
  for word in vocabulario:
    for n in titulos:
      TF_vector.append(n.count(word))

  print(""----------------------Literal 3----------------------"")
  TF_matriz=np.reshape(TF_vector,(len(vocabulario),len(titu)))
  print(TF_matriz)
  print(""Bolsa de palabras unificada (tanto de consultas como documentos):"",TF_matriz.shape)

  ##########CALCULO DE WTF##########
  for item in TF_vector:
    if item>0:
      factor_WTF = 1+(math.log10(item))
      WTF_vector.append(factor_WTF)
    else:
      WTF_vector.append(0)

  WTF_matriz=np.reshape(WTF_vector,(len(vocabulario),len(titu)))

  ##########CALCULO DE DF##########
  for word in vocabulario:
    i=0
    for item in range(0,len(titu)):
      if word in titulos[item]:
        i += 1
    DF_vector.append(i)

  ##########CALCULO DEL IDF##########
  for item in DF_vector:
    factor_ITF = math.log10(len(titu)/item)
    IDF_vector.append(factor_ITF)


  ##########CALCULO WTFxIDF##########
  for x in range(len(vocabulario)):
    for y in range(len(titu)):
      WTDxIDF_vector.append(IDF_vector[x]*WTF_matriz[x][y])

  WTDxIDF_matriz=np.reshape(WTDxIDF_vector,(len(vocabulario),len(titu)))
  #4. Obtener la matriz TF-IDF, de toda la bolsa total de palabras (1 Ptos.)
  print(""----------------------Literal 4----------------------"")
  print(WTDxIDF_matriz)
  print(""Dimensiones de la Matriz TF-IDF:"",WTDxIDF_matriz.shape)

  ##########NORMALIZACION DE VECTORES##########
  for i in range(len(titu)):
      acum  = 0
      for fila in WTDxIDF_matriz:
          acum = acum + pow(fila[i],2)
      vectorSumaColumnas.append(pow(acum,1/2))

  for x in range(len(vectorSumaColumnas)):
    for y in range(len(WTDxIDF_matriz)):
      vectorNormalizado.append(WTDxIDF_matriz[y][x]/vectorSumaColumnas[x])     

  matrizNormalizada = np.reshape(vectorNormalizado,(len(vectorSumaColumnas),len(WTDxIDF_matriz)))
  matrizTranspuesta = np.transpose(matrizNormalizada)

  TRANSPUESTA_dataframe=pd.DataFrame(matrizTranspuesta)

  ##########MTODO DEL SIMILITUD COSENO##########
  for i in range(len(titu)):
    for j in range(len(titu)):
      if j>=i:
        factor = round(TRANSPUESTA_dataframe[i].dot(TRANSPUESTA_dataframe[j]),3)
        valores.append(factor)
      else:
        valores.append(0)
      
  valores = np.reshape(valores,(len(titu),len(titu)))
  valores = valores + valores.T - np.diag(np.diag(valores))

  return valores
#######################################MATRIZ DE SIMILITUD CON DESIGNACIN DE PESOS#######################################
def similitud(matrizTitulos,matrizKeywords,matrizAbstract):
  #PESO PARA TTULOS 20% (0.2)
  titulosPeso = np.multiply(matrizTitulos,0.2)
  #PESO PARA KEYWORDS 30% (0.3)
  keywordsPeso = np.multiply(matrizKeywords,0.3)
  #PESO PARA ABSTRACT 50% (0.5)
  abstractPeso = np.multiply(matrizAbstract,0.5)
  
  matrizSimilitudPrevia = np.add(titulosPeso,keywordsPeso)
  matrizSimilitud = np.add(matrizSimilitudPrevia,abstractPeso)

  SIMILITUD_dataframe=pd.DataFrame(matrizSimilitud)

  return SIMILITUD_dataframe

#MAIN DEL EXAMEN
[titulos,d,vocabulario,titu] = normalizacion(""dato"")
matrizDatoJaccard = metodoJaccard(titulos,d)
matrizDatoCoseno = metodoCoseno(vocabulario,titu,titulos)
#5. Qu tan similares son los documentos D4 y D1. Exprese su respuesta como una probabilidad utilizando dos mtodos diferentes (1 Pto.)
print(""----------------------Literal 5----------------------"")
print(""Probabilidad de similitud entre D4 y D1 mediante el Mtodo de Jaccard:"",matrizDatoJaccard[3][0])
print(""Probabilidad de similitud entre D4 y D1 mediante el Mtodo del Coseno:"",matrizDatoCoseno[3][0])
#6. Considerando un sistema de RI. Si un usuario realiza la tercera consulta, cul debera ser el orden en el que se desplieguen la coleccin de ocho documentos en funcin de 
#su importancia?.  Justifique su respuesta mediante un sistema de puntuacin , donde se muestre los resultados de cada puntuacin (2 Ptos.)
print(""----------------------Literal 6----------------------"")
print(""Puntuacin Jaccard"")
puntuacion_D3_jaccard = matrizDatoJaccard[10,:8]
ranking1 = np.sort(puntuacion_D3_jaccard)
print(np.sort(ranking1))
print(""1. Documento 5:"",ranking1[-1])
print(""2. Documento 8:"",ranking1[-2])
print(""3. Documento 7:"",ranking1[-3])
print(""4. Documento 2:"",ranking1[-4])
print(""5. Documento 6:"",ranking1[-5])
print(""6. Documento 1:"",ranking1[-6])
print(""7. Documento 3:"",ranking1[-7])
print(""8. Documento 4:"",ranking1[-8])
print(""Puntuacin Coseno"")
puntuacion_D3_coseno = matrizDatoCoseno[10,:8]
ranking2 = np.sort(puntuacion_D3_coseno)
print(np.sort(ranking2))
print(""Orden de similitud segn Jaccard"")
print(""1. Documento 8:"",ranking2[-1])
print(""2. Documento 5:"",ranking2[-2])
print(""3. Documento 7:"",ranking2[-3])
print(""4. Documento 6:"",ranking2[-4])
print(""5. Documento 2:"",ranking2[-5])
print(""6. Documento 1:"",ranking2[-6])
print(""7. Documento 3:"",ranking2[-7])
print(""8. Documento 4:"",ranking2[-8])

print(""Tiempo total de ejecucin: %s segundos"" % (time.time() - start_time))"
